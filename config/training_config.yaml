# Model configuration
model:
  name: "microsoft/Phi-4-vision-32k"
  gradient_checkpointing: true
  lora_config:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    bias: "none"

# Training hyperparameters
training:
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 500
  gradient_clip_val: 1.0
  gradient_accumulation_steps: 4
  early_stopping_patience: 3
  val_check_interval: 0.25

# Data processing
data:
  train_batch_size: 8
  eval_batch_size: 8
  num_workers: 4
  max_length: 2048
  debug_mode: false

# Trainer settings
trainer:
  max_epochs: 10
  gpus: -1  # Use all available GPUs
  precision: "bf16"  # Use bfloat16 precision for faster training

# Logging settings
logging:
  log_every_n_steps: 10
  use_wandb: true
  wandb_project: "phi4-medical-vqa"
  wandb_run_name: "phi4-lora-finetuning"

# Output directories
output:
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"
  prediction_dir: "./predictions"